
Lecture 2:

Locality of Reference:
Big data systems need to move large volumes of data
   -- To reduce the latency bring the data closer to the compute.

It is the tendency of processor to access the same set of memory locations repetatively.

Hit Ratio - The performance of cache
  Hit + Miss = Total cpu reference
  Hit Ratio = Hit/(Hit+Miss)

Access Time of Memories
Tavg = h * Tc + (1-h)*(Tm+Tc)
h - hit ratio
Tc - Time to access cache
Tm - Time to access main memory

Temporal locality: Data that is accessed is likely to be accessed again in near future.
        ex : Instructions in the body of loop, local variables, A recent social media post, Netflix region

Spatial locality: Data accessed is likely located adjacent to data that is to be accessed in near future.
        ex : Arrays, linear sequence of instructions,

*******************************************************************************************************************
Lecuture 3:

UMA(Uniform Memory Access) - Every processor is connected to a common memory
NUMA(Non Uniform Memory Access) - Each processor has its own memory

Interconnection Networks between memorey and cpu
crossbar switch - faster, dedicated line
Omega switch - cheaper, crossbar shared switches

Flynn's Taxanomy:
SISD - Single Instruction Single Data
SIMD - Single Instruction Multiple Data
MISD - Multiple Instuction Single Data
MIMD - Multiple Instruction Multiple Data

High parallelism may not lead to high speed - Depends on granularity.
Granularity - Average number of compute instructions before communication is needed across processor.
If more commnunication is required, Then its good to go with multi processor/computer systems.
If less communication then go with distributed systems.

Amhdal's Law   ---> speed is limited by sequential part of the program.
• T(1) : Time taken for a job with 1 processor
• T(N) : Time taken for same job with N processors
• Speedup S(N) = T(1) / T(N)
• S(N) is ideally N when it is a perfectly parallelizable program, i.e. data parallel with no sequential component
• Assume fraction of a program that cannot be parallelised (serial) is f and 1-f is parallel
• S(N) = T(1) / ( f * T(1) + (1-f) * T(1) / N )   Only parallel portion is faster by N
• S(N) = 1 / ( f + (1-f) / N )

10% of a program is sequential and there are 100 processors. What is the effective speedup ?
S(N) = 1 / ( f + (1-f) / N )
S(100) = 1 / ( 0.1 + (1-0.1) / 100 ) = 1 / 0.109
= 9.17 (approx)

Amdhal's law doesn't consider network/communication delay, context switch, I/O etc..

Gustafson-Barsis Las --- Solver larger problems when you have more processors. Increase the workload with increase in number of processor.
This law is about how much workload can be processed with N processor.

Let W be the execution workload of the program before adding resources f is the sequential part of the workload
So W = f * W + (1-f) * W
Let W(N) be larger execution workload after adding N processors
So W(N) = f * W + N * (1-f) * W
Parallelizable work can increase N times
The theoretical speedup in latency of the whole task at a fixed interval time T S(N) = T * W(N) / T * W
= W(N) / W = ( f * W + N * (1-f) * W) / W
S(N) = f + (1-f) * N
S(N) is not limited by f as N scales


Data Access strategies

1. Replication - replicate all data across nodes of the distributed system
    - Higher storage cost
    - All data accessed from local disk so no runtime communication on network
    - High performance with parallel process
    - Fail over across replicas
  concerns: Keep replicas in sync - various consistency models between readers and writers.

2. Partition - partition the data typically equally to all the nodes of distributed systems
     - High query cost if query needs to access multiple partitions.
     - Works well for tasks/algo is data parallel.
     - Works well when there is locality of reference within partition.
  concerns: 
     - Partition balancing
     - sharding problems
     - How to improve locality of reference

3. Dynamic communication : Communicate only the data that is required.
    cost : high network cost for loosely couupled systems and data set to be exchanged is large
   Advantage: Minimal communication cost
   concern: Highly available and performant network

4. Networked storage: Common storage on the network (SAN, NAS)
      common storage on the cloud - Amazon s3

*******************************************************************************************************************
Lecture 4

Cluser computing
- Peep to peer computing
- clinet server computing
- Cloud and distributed computing
Hadoop cluster architecture


Data Analytics:
  -- Used to analyse the structured and unstructured data to derive meaningfule business advantages.

Descriptive analysis : Provides ability to alert, explore and report mostly using internal and external data
   - Usually data from legacy systems, ERP CRM
   - Relational data from data warehouse
   - Structred and not very large data sets
   Solves questions like
    What happened? and why happened?
    Analytics 1.0

Predictive Analysis: Use the data to predict the future
    -- Based on large, unstructred data gathered over period of time
    -- Analytics 2.0

Prescriptive Analysis : Which use data from past to make predictions and also recommendations
    -- Data is blend from big data and legacy systems
    -- Analytics 3.0 - Descriptive + predictive + prescriptive
    -- What, when, why it will happen and what is the action to be taken?

Big Data analtics : 
    -- Wortking with datasets with huge volume, variety and velocity beyond storage capabilities of RDBMS
    -- Locality of reference - move code closer to data
    -- To gain competive advantage
    -- Batch processing and stream processing.
    -- Lot of technologies available
What is not Big Data system
 * Not to meant to replace RDBMS
 * Depending on usecase need to develop techstack and build big data pipeline.
 * Not a volume game

Adoption challenges:
   -- Executive sponsership for big data analysis activities
   -- Sharing the data across different business units.
   -- Right skillset
   -- To use structured or unstructured data
   -- Storage and processing capbilities - In house or cloud ?
   -- Determining what to do with insights created from big data.

Technology challenges:
   -- Scale - horizontal scaling
   -- Security 
   -- Schema : need to have dynamic schema
   -- Continuous Availability
   -- consistency
   -- partion tolerant
   -- Data quality

*******************************************************************************************************************
Lecture 5

Issues with Map Reduce
- Latency is high as reading and writing scripts from disks
- Poor filt for Iterative applications such as ML and graph algo
- Not in memory computing. Spark is in memory computing.

Reliability - An inverse indicator of failure rate. How soon system will fail.

MTTF - Mean Time To Failure is an Averaged value. 
      Total number of failures / Total number of units
Failure Rate = 1/MTTF
Increase in the MTTF value is the objective.

MTTR - Mean Time to Recovery
     - Total number of hours of maintainance/ Total number of repairs

MTTD - Mean Time to Diagnose
MTBF - Mean Time Between Failures - MTTD + MTTR + MTTF

MTTF reliablity
Serial Assembly system - C = A + B
MTTF of system = 1/SUM(1/MTTFi) for all components i
Failure rate = SUM(1/MTTFi) for all components i

Parallel Assembly system = C = A or B
MTTF of C = MTTF of A + MTTF of B  because both A and B have to fail for C to fail
MTTF of system = SUM(MTTFi) for all components i
Failure rate = 1/SUM(MTTFi) for all component i

MTTF availability
Availability = Time system is UP and accessible/Total time observer
Availablity = MTTF/(MTTD* + MTTR + MTTF) - Unless specified MTTD is zero.
            = MTTF/MTBF
System is highly available when
MTTF is high
MTTR is low

Problem:
A node is cluster failed every 100 hour while other parts never fail. On failure of the node whole system needs to be shutdown, faulty node replaced and system.
This takes 2 hours, The application needs to be restarted which takes 2 hours.

what is availabilty?
If downtime is $80k per hour, what is the yearly cost?
Answer - This is serial assembly
        MTTF = 1/(1/100)
             = 100

        Availablity = 100/ 100 + 2 + 2
                    = 0.961

         yearly cost = 1 - 0.961 = 0.039
                     for 100 hrs/24 hrs = 4.16 days
                      for month 30/4.16 = 7.211 - 7.211 * 0.039 = 0.28125
                      for year 0.28125 * 12 = 3.375 = 3.375 * 80K = $270k
                     
                

Fault tolernace configurations - 

Load Balanced : Active - Active - costly
Hot standby : Active - passive - One is active and other is standby - Takes few seconds
Warm standby : Software installed on secondary and up and running, Once Active is down seconday is configured - few minutes
Cold standby : Secondary node act as backup for an identical primary system - Secondary is installed and configured after active fails - Takes few hours.

Fault tolerant cluster recovery
Diagnosis - heart beat messages
Backward recovery - Rollback to a particular checkpoint.
Forward recovery - Reconstruct the state based on the data.

CAP - Theorem
    A distyibuted system running over a cluster can only provide 2 of the 3 properties.
    In effect, When there is partition, The system has to decide whehter to pick consistency and availability.
   i.e CP or AP because partion is bound to happen in distributed systems.
Consistency: A read of data item from any node results in same data across multiple nodes

Availabilty: A read/write request always acknowledged in form of succes or failure in reasonable time

Partion Tolerance: System can continue to function when communication outages split the cluster into multiple silo and can still service read/write requests

BASE - Base is a database design principle based on CAP theorem.
statnds for 
Basically Available - always repsonsds with success or failure
Soft state - State where db node is not in sync with other nodes.
Eventual consistency - State in which reads by different clients, immediately following a wrtie to database may not return consistent results.



*******************************************************************************************************************
Lecture 6

Level of Consistency

Strict - A read of data item from any node results in same data across all nodes

Linearisable - Allows read by other client while that value is being updated.

Sequential -  Updates to a values should be read by other nodes sequentially. It means x is updated 1st and Y is updated next, Then node reading X and Y should get updated value of X first and then Y.

Causal - Node sees writes to x and then writes to y. Means only casually connected to writes and reads need to be ordered. Example value of X is used to write Y. So all the nodes must observer write to x first then writes to y.

Eventual - If there are no writes for some time then all threads agree on latest value of data item.


Majority RW - Read majority of nodes or write majority of nodes to get causal consistency.

Different types of consistency models
 When Network partition happens there can be many secnarios for fault tolerance. Below are some of the use cases that can be configured in DB / big data systems based on usecase.
Example, Cluster has 9 nodes and due to network partion cluster has divided in to 2 partition one with 5 nodes and other with 4 nodes with new primary node of new cluster.
- causal consistency with durable writes -  Read and write happens with majority of clusters, Read/write to non majority cluster should faile.
- causal consistency with non durable writes - Read happens with majority cluster but write will be succeeded for 1(so it will immediately succeed on non majority cluster) - This write may be rolled back when immediate read operation is done on majority nodes.
- eventual consisitency with durable wirtes - Writes may not be seen immediately but eventually. read local and write majority
- eventual consistency but no durablitiy - read local and write 1 - Again this write may be rolled back as is the case with causal consistency with no durable wirtes.


Big data analytics life cycle
1. Business case Evaluation
2. Data Identification
3. Data acquisiton and filtering
4. Data extraction
5. Data valiation and cleansing
6. Data aggregation and represenations
7. Data analysis
8. Data visualisation
9. Untilisation of analysis results

*******************************************************************************************************************
Lecture 7

TOP DOWN DESIGN 
   -   Sequential - f1 ->f2->f3
   -   Parallel   - f1 ->f2   
                              f4
                       -> f2


Data parallelism - Static partion is done and assign it to number of available processor. Here data partion may be balanced or un balanced.
Tree level parallelism - Divide problem in to subproblem in tree strucutre and assign it to those many processors. (Quick sort)

Task parallelism - Parallel tasks that work on same data

Request parallelism - Scalable execution of independent tasks in parallel. Execute same code but in parallel instances (Client server models)

Some MAP REDUCE Examples

*******************************************************************************************************************
Lecture 8

HADOOP Architecutre 

client server and peep to peer communication

master node and slave nodes

Master Node
   - Name Node - HDFS name space metadata manager
   - YARN cluster level resource manager

Slave Node
   - Data Node - Node level data manager - Stores actual data
   - Node manager - YARN node level resource manager
   - MAP, REDUCE tasks

HDFS features 
   - Distributed java based file system that sits on top of native FS
   - Data is split in to large blocks : 128MB
   - Enables storage of very large files across nodes of Hadoop cluster
   - Scale through parallel data processing
      - 1 node with 1 TB storage with IO bandwidth of 400 mb/s vs 10 node with 10 GB storage with 400mb/s - Second one is faster and scalable
   - Fault tolerant through replication - default 3 replicas
   - Consistency : Write once and read many times
   - cost : Adding more nodes is cost effective
   - Variety and volume of Data - structured, unstructured, semi structured and Hude Terabyte and peta bytes of data
   - Data integrity : Verifys checksum to preserve data integrity.
   - Data locality : Bring computation closer to the data

Client contacts Namenode on Master node and namenode checks meta data and responsds back with Data node information where client has to connect to based on location or availability of data nodes.

NameNode:
   FsImage - contains mapping of blocks to file, hierararchy and file operations/operations
   edit logs - Transaction log of changes to metadata in FsImage - records each change

   - Does not store any data - only meta data
   - HA can be configured
   - Receives heartbeat and block report from all Data nodes in cluster
   - Ensure replication factor is maintained.

   On Startup - 
      - check for status of Datanodes on slaves
      - Gets hearbeat and block report from data nodes
      - check the replication factor
      - Updates meta data
      - Reads FSImages and editlogs from dist to memory
      - Applies edit log changes to FSImages

Secondary Name node - constantly reads all the file system and metadata from RAM of the NameNode and writes to its local file system.
                     - Downloads Edit logs from namenodes and applies it FSImage on regualr check points.
                     - Thhis node takes over when primary is down.
                     - Not Hot standby

HA Config of Name node
   -   Data node sends hearbeat to both name nodes
   - Zookeeper session used for failure detection and election of new active.
   - Journal Nodes/NFS - Write happens Only via Active node then passive name node will read from Journal Node.

Replica placement strategy with Rack awareness
   -   First replica is placed on the same node as client
   -    Second replica is placed on node that is present on different rack
   -   Third replica is places as second but on a different node
   -    For replica factor greater than 3 - 4th replica can be placed randomly which satisfies upper limit per rack 

*******************************************************************************************************************
Lecture 9

MAP REDUCE - Write pipelines and read pipeline

   -   Client sends request for read/write 
   -   Name node receives the request and check its metadata and fetch the datanodes where this block belongs.
   -   Name node then sends these datanodes info to client (DN1, DN2 and DN3 in case of 3 replicas)
   -   client then creates TCP/IP conection to lets say DN1.
   -   client informs DN1 to be ready to receive the block and provides IP of DN2 and DN3 for replication.
   -   With this pipeline will be setup from client to for example DN1 then DN1 to DN2 and then DN2 to DN3.
   -   Then client will start transferring the data to DN1, Then DN1 replicates the data to DN2 and then DN2 replicates data to DN3.
   -   Once the data is transferred, Datanode will update client that data written successfully in reverse manner like DN3-->DN2-->DN1-->client
   -   Client inform namenode that data has been written successfully
   -   Namenode updates metadata and then client shutdowns connection.

Data can be written parallely by setting up multiple pipelines. For ex one pipeline for each block request.
Read request are served by chosing namenodes which is closes to client. This reduces read latency. Reads are parallel.


MapReduce program
   -   RecordReader - reads each record created by input splitter to pass key-valu pair to map
   -   Map : userdefined function to create key, value pair from input key, value pair
   -   combiner: optional reducer for better performance - can be same as reduced code.
   -   partitioner: Takes intermediate output from map and shards it to different reducers.
            -   Ensuers same key is sent to same reducer.
   -   Reduce
      -   shuffle and sort
      -   Reducer

*******************************************************************************************************************
Lecture 10

YARN - Yet Another Resource Negotiotor

 - Key role is to schedule resources in the cluster
 - Takes request from client and talks to node managers for allocation
- creates appmaster
- client can contact appmaster

AppMaster
   - Negoritates resources from resouce manager per application for starting container on nodes.
   - Sends periodic health status of application containers and track progress
   - Talks via Node Manager for updates and usage reports to resource manager
   - Clients can directly talk to appmaster

conatier

Workflow
1. client submits the application/job with specs to start AppMaster
2. RM asks nodemanager to start container and then launches AppMaster  
3. AppMaster on startup registers with ResourceManager(RM). Now client can directly contact to AppMaster
4. As the app executes, The AppMaster negotiotes resouces in the form of conainters vai the resource request protocol invlolving RM
5. As a container is allocated successfully for an app, AppMaster works with the nodemanager on same of diff node to launch the container.
6. App specific code insided the container provides runtime information to the appmaster for progress, status etc..
7. Client can directly communicate with appMaster for progress
8. On complettion of job AppMaster deregisters from RM and shuts down so that containers allocated can be re-purposed.

Capacity scheduler
min capacity, max capacity, user limit factor   -- Go thru the slides

*******************************************************************************************************************
Lecture 11 - Hadoop technology ecosystems

PIG - Gives developer a simpler scripting interface to manipulate data using a higher level programming interface
    - similar to sql
    - pig script is translated to map reduce tasks by the pig runtime and executed on hadoop cluster

    - programs are written in Pig Latin - scripting language

PIG Data model
    Atom - Basic data type like byte, int

    Tuple -  Ordered set of fields of various types, like a row in RDBMS table
      eg : (andy, 25)

    Bag : An unordered set of tuples. A bag can be inside tuple
      eg : {(andy,65), (ram, 23, 6000)}
       inner bag : (1, {(1,2,3)})

    Map : key value pair
      eg : [name#andy, age#25]

    Relation: A bag but not an inner bag which can be inside a tuple. Like table in RDBMS.

HBASE : Key value store
  HDFS provides sequential access to data
  HBASE provides random access capability of large files in HDFS. Hash tables, key value stores
  Built for wide tables with many attributes
  column oriented storage.
  strongly consistent
  can be used when need to lookup data in large data store.

  columnar storage - to leverage spatial locality
    - Is about grouping multiple columns in to column family

HIVE: Data warehouse
   Provides way to process large structured data in hadoop.
   Data warehouse on Hadoop/HDFS
   SQL like query interface - HiveQL
      -   Its not RDBMS
      -   Not for real-time queries with row level updates
   Meant for OLAP type quereies
   Contains execution engine which transaltes Hive query in to map reduce jobs

SQOOP - SQL to Hadoop and Hadoop to SQL
         Cannot ingest continuous stream of data.
         Its limited to structured data.

FLUME - To move large non relational data : streaming data eg. logs
         Event driven
         More reliable

Zookeeper:
   Provides coordination system

Oozie : Scheduler for hadoop jobs
   It maintains metadata.
   Check Master is working fine or not

*******************************************************************************************************************
SQL vs NoSql Database
Mongo db - practice queries
Graph database - Neo4j practice queries - November 3 video
Cloud service for big data - November 5 video (Mostly repetation on cloud IAAS, PAAS, SAAS, Lambda, VPC, s3 object storage, EFS, EBS

Big data computing engines - November 6 video
Spark
- Hadoop is slower because of read/write to disks - Suitable for batch processing
- Spark is inmemory, Works on RDD engine - Resilient Distributed Dataset
- Do not use spark for large batch processes with high memore requirements

map() – Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.
flatMap() – Spark flatMap() transformation flattens the DataFrame/Dataset after applying the function on every element and returns a new transformed Dataset. The returned Dataset will return more rows than the current DataFrame. It is also referred to as a one-to-many transformation function. This is one of the major differences between flatMap() and map()
Key points

Both map() & flatMap() returns Dataset (DataFrame=Dataset[Row]).
Both these transformations are narrow meaning they do not result in Spark Data Shuffle.
flatMap() results in redundant data on some columns.
One of the use cases of flatMap() is to flatten column which contains arrays, list, or any nested collection(one cell with one value).
map() always return the same size/records as in input DataFrame whereas flatMap() returns many records for each record (one-many).

Example1:- 
sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()
Output:
[[1, 2], [1, 2, 3], [1, 2, 3, 4]]

sc.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()
Output:  notice o/p is flattened out in a single list
[1, 2, 1, 2, 3, 1, 2, 3, 4] 

*******************************************************************************************************************
Machine Learning

K means clustering:

Euclidean Distance Formula

d = squarerootOf (x2 - x1)^2 + (y2 - y1)^2
x2 - observed value
x1 - centroid value

y2 - observed value
y1 - centroid value

Dataset
Candidate	Weight	Glucose level
1             	72 	185 
2             	56 	170 
3 	            60 	168 
4 	            68 	179 
5 	            72 	182 
6 	            77 	188 
7             	70 	180 
8 	            84 	183 

calculating K means where k = 2

Initial centroid value k1 = C1 - (72, 185)
Initial centroid value k2 = C2 - (56, 170)

Applying formula:
calculating centroid value for C3

w.r.t K1
C3 = squarerootof ((60-72)^2 + (168 - 185)^2)
   = squarerootof (144 + 289)
   = 20.80

w.r.t K2
C3 = squarerootof (60-56)^2 + (168-170)^2
   = squrerootof ( 16 + 4)
   = 4.4721

4.47 is lesser, So C3 will go into k2 cluster.

Now, K1 - (C1)  K2 - (C2, C3)
Update the value of K2 since it has new candidate.

New K2 will be - (C2+c3)/2 = (56 + 60)/2 and (168 + 170)/2 = 55, 169

so, K1 - (72, 185)  K2 - (55, 169)

Further calulation like C4 will be based on above values.
