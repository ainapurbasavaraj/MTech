
Lecture 2:

Locality of Reference:
Big data systems need to move large volumes of data
   -- To reduce the latency bring the data closer to the compute.

It is the tendency of processor to access the same set of memory locations repetatively.

Hit Ratio - The performance of cache
  Hit + Miss = Total cpu reference
  Hit Ratio = Hit/(Hit+Miss)

Access Time of Memories
Tavg = h * Tc + (1-h)*(Tm+Tc)
h - hit ratio
Tc - Time to access cache
Tm - Time to access main memory

Temporal locality: Data that is accessed is likely to be accessed again in near future.
        ex : Instructions in the body of loop, local variables, A recent social media post, Netflix region

Spatial locality: Data accessed is likely located adjacent to data that is to be accessed in near future.
        ex : Arrays, linear sequence of instructions,

*******************************************************************************************************************
Lecuture 3:

UMA(Uniform Memory Access) - Every processor is connected to a common memory
NUMA(Non Uniform Memory Access) - Each processor has its own memory

Interconnection Networks between memorey and cpu
crossbar switch - faster, dedicated line
Omega switch - cheaper, crossbar shared switches

Flynn's Taxanomy:
SISD - Single Instruction Single Data
SIMD - Single Instruction Multiple Data
MISD - Multiple Instuction Single Data
MIMD - Multiple Instruction Multiple Data

High parallelism may not lead to high speed - Depends on granularity.
Granularity - Average number of compute instructions before communication is needed across processor.
If more commnunication is required, Then its good to go with multi processor/computer systems.
If less communication then go with distributed systems.

Amhdal's Law   ---> speed is limited by sequential part of the program.
• T(1) : Time taken for a job with 1 processor
• T(N) : Time taken for same job with N processors
• Speedup S(N) = T(1) / T(N)
• S(N) is ideally N when it is a perfectly parallelizable program, i.e. data parallel with no sequential component
• Assume fraction of a program that cannot be parallelised (serial) is f and 1-f is parallel
• S(N) = T(1) / ( f * T(1) + (1-f) * T(1) / N )   Only parallel portion is faster by N
• S(N) = 1 / ( f + (1-f) / N )

10% of a program is sequential and there are 100 processors. What is the effective speedup ?
S(N) = 1 / ( f + (1-f) / N )
S(100) = 1 / ( 0.1 + (1-0.1) / 100 ) = 1 / 0.109
= 9.17 (approx)

Amdhal's law doesn't consider network/communication delay, context switch, I/O etc..

Gustafson-Barsis Las --- Solver larger problems when you have more processors. Increase the workload with increase in number of processor.
This law is about how much workload can be processed with N processor.

Let W be the execution workload of the program before adding resources f is the sequential part of the workload
So W = f * W + (1-f) * W
Let W(N) be larger execution workload after adding N processors
So W(N) = f * W + N * (1-f) * W
Parallelizable work can increase N times
The theoretical speedup in latency of the whole task at a fixed interval time T S(N) = T * W(N) / T * W
= W(N) / W = ( f * W + N * (1-f) * W) / W
S(N) = f + (1-f) * N
S(N) is not limited by f as N scales


Data Access strategies

1. Replication - replicate all data across nodes of the distributed system
    - Higher storage cost
    - All data accessed from local disk so no runtime communication on network
    - High performance with parallel process
    - Fail over across replicas
  concerns: Keep replicas in sync - various consistency models between readers and writers.

2. Partition - partition the data typically equally to all the nodes of distributed systems
     - High query cost if query needs to access multiple partitions.
     - Works well for tasks/algo is data parallel.
     - Works well when there is locality of reference within partition.
  concerns: 
     - Partition balancing
     - sharding problems
     - How to improve locality of reference

3. Dynamic communication : Communicate only the data that is required.
    cost : high network cost for loosely couupled systems and data set to be exchanged is large
   Advantage: Minimal communication cost
   concern: Highly available and performant network

4. Networked storage: Common storage on the network (SAN, NAS)
      common storage on the cloud - Amazon s3

*******************************************************************************************************************
Lecture 4

Cluser computing
- Peep to peer computing
- clinet server computing
- Cloud and distributed computing
Hadoop cluster architecture


Data Analytics:
  -- Used to analyse the structured and unstructured data to derive meaningfule business advantages.

Descriptive analysis : Provides ability to alert, explore and report mostly using internal and external data
   - Usually data from legacy systems, ERP CRM
   - Relational data from data warehouse
   - Structred and not very large data sets
   Solves questions like
    What happened? and why happened?
    Analytics 1.0

Predictive Analysis: Use the data to predict the future
    -- Based on large, unstructred data gathered over period of time
    -- Analytics 2.0

Prescriptive Analysis : Which use data from past to make predictions and also recommendations
    -- Data is blend from big data and legacy systems
    -- Analytics 3.0 - Descriptive + predictive + prescriptive
    -- What, when, why it will happen and what is the action to be taken?

Big Data analtics : 
    -- Wortking with datasets with huge volume, variety and velocity beyond storage capabilities of RDBMS
    -- Locality of reference - move code closer to data
    -- To gain competive advantage
    -- Batch processing and stream processing.
    -- Lot of technologies available
What is not Big Data system
 * Not to meant to replace RDBMS
 * Depending on usecase need to develop techstach and build big data pipeline.
 * Not a volume game

Adoption challenges:
   -- Executive sponsership for big data analysis activities
   -- Sharing the data across different business units.
   -- Right skillset
   -- To use structured or unstructured data
   -- Storage and processing capbilities - In house or cloud ?
   -- Determining what to do with insights created from big data.

Technology challenges:
   -- Scale - horizontal scaling
   -- Security 
   -- Schem : need to have dynamic schema
   -- Continuous Availability
   -- consistency
   -- partion tolerant
   -- Data quality

*******************************************************************************************************************
Lecture 5

Issues with Map Reduce
- Latency is high as reading and writing scripts from disks
- Poor filt for Iterative applications such as ML and graph algo
- Not in memory computing. Spark is in memory computing.

Reliability - An inverse indicator of failure rate. How soon system will fail.

MTTF - Mean Time To Failure is an Averaged value. 
      Total number of failures / Total number of units
Failure Rate = 1/MTTF
Increase in the MTTF value is the objective.

MTTR - Mean Time to Recovery
     - Total number of hours of maintainance/ Total number of repairs

MTTD - Mean Time to Diagnose
MTBF - Mean Time Between Failures - MTTD + MTTR + MTTF

MTTF reliablity
Serial Assembly system - C = A + B
MTTF of system = 1/SUM(1/MTTFi) for all components i
Failure rate = SUM(1/MTTFi) for all components i

Parallel Assembly system = C = A or B
MTTF of C = MTTF of A + MTTF of B  because both A and B have to fail for C to fail
MTTF of system = SUM(MTTFi) for all components i
Failure rate = 1/SUM(MTTFi) for all component i

MTTF availability
Availability = Time system is UP and accessible/Total time observer
Availablity = MTTF/(MTTD* + MTTR + MTTF) - Unless specified MTTD is zero.
            = MTTF/MTBF
System is highly available when
MTTF is high
MTTR is low

Problem:
A node is cluster failed every 100 hour while other parts never fail. On failure of the node whole system needs to be shutdown, faulty node replaced and system.
This takes 2 hours, The application needs to be restarted which takes 2 hours.

what is availabilty?
If downtime is $80k per hour, what is the yearly cost?
Answer - This is serial assembly
        MTTF = 1/(1/100)
             = 100

        Availablity = 100/ 100 + 2 + 2
                    = 0.961

         yearly cost = 1 - 0.961 = 0.039
                     for 100 hrs/24 hrs = 4.16 days
                      for month 30/4.16 = 7.211 - 7.211 * 0.039 = 0.28125
                      for year 0.28125 * 12 = 3.375 = 3.375 * 80K = $270k
                     
                

Fault tolernace configurations - 

Load Balanced : Active - Active - costly
Hot standby : Active - passive - One is active and other is standby - Takes few seconds
Warm standby : Software installed on secondary and up and running, Once Active is down seconday is configured - few minutes
Cold standby : Secondary node act as backup for an identical primary system - Secondary is installed and configured after active fails - Takes few hours.

Fault tolerant cluster recovery
Diagnosis - heart beat messages
Backward recovery - Rollback to a particular checkpoint.
Forward recovery - Reconstruct the state based on the data.

CAP - Theorem
    A distyibuted system running over a cluster can only provide 2 of the 3 properties.
    In effect, When there is partition, The system has to decide whehter to pick consistency and availability.
   i.e CP or AP because partion is bound to happen in distributed systems.
Consistency: A read of data item from any node results in same data across multiple nodes

Availabilty: A read/write request always acknowledged in form of succes or failure in reasonable time

Partion Tolerance: System can continue to function when communication outages split the cluster into multiple silo and can still service read/write requests

BASE - Base is a database design principle based on CAP theorem.
statnds for 
Basically Available - always repsonsds with success or failure
Soft state - State where db node is not in sync with other nodes.
Eventual consistency - State in which reads by different clients, immediately following a wrtie to database may not return consistent results.



*******************************************************************************************************************
Lecture 6

Level of Consistency

Strict - A read of data item from any node results in same data across all nodes

Linearisable - Allows read by other client while that value is being updated.

Sequential -  Updates to a values should be read by other nodes sequentially. It means x is updated 1st and Y is updated next, Then node reading X and Y should get updated value of X first and then Y.

Causal - Node sees writes to x and then writes to y. Means only casually connected to writes and reads need to be ordered. Example value of X is used to write Y. So all the nodes must observer write to x first then writes to y.

Eventual - If there are no writes for some time then all threads agree on latest value of data item.


Majority RW - Read majority of nodes or write majority of nodes to get causal consistency.

Different types of consistency models
 When Network partition happens there can be many secnarios for fault tolerance. Below are some of the use cases that can be configured in DB / big data systems based on usecase.
Example, Cluster has 9 nodes and due to network partion cluster has divided in to 2 partition one with 5 nodes and other with 4 nodes with new primary node of new cluster.
- causal consistency with durable writes -  Read and write happens with majority of clusters, Read/write to non majority cluster should faile.
- causal consistency with non durable writes - Read happens with majority cluster but write will be succeeded for 1(so it will immediately succeed on non majority cluster) - This write may be rolled back when immediate read operation is done on majority nodes.
- eventual consisitency with durable wirtes - Writes may not be seen immediately but eventually. read local and write majority
- eventual consistency but no durablitiy - read local and write 1 - Again this write may be rolled back as is the case with causal consistency with no durable wirtes.


Big data analytics life cycle
1. Business case Evaluation
2. Data Identification
3. Data acquisiton and filtering
4. Data extraction
5. Data valiation and cleansing
6. Data aggregation and represenations
7. Data analysis
8. Data visualisation
9. Untilisation of analysis results


