
Lecture 2:

Locality of Reference:
Big data systems need to move large volumes of data
   -- To reduce the latency bring the data closer to the compute.

It is the tendency of processor to access the same set of memory locations repetatively.

Hit Ratio - The performance of cache
  Hit + Miss = Total cpu reference
  Hit Ratio = Hit/(Hit+Miss)

Access Time of Memories
Tavg = h * Tc + (1-h)*(Tm+Tc)
h - hit ratio
Tc - Time to access cache
Tm - Time to access main memory

Temporal locality: Data that is accessed is likely to be accessed again in near future.
        ex : Instructions in the body of loop, local variables, A recent social media post, Netflix region

Spatial locality: Data accessed is likely located adjacent to data that is to be accessed in near future.
        ex : Arrays, linear sequence of instructions,

*******************************************************************************************************************
Lecuture 3:

UMA(Uniform Memory Access) - Every processor is connected to a common memory
NUMA(Non Uniform Memory Access) - Each processor has its own memory

Interconnection Networks between memorey and cpu
crossbar switch - faster, dedicated line
Omega switch - cheaper, crossbar shared switches

Flynn's Taxanomy:
SISD - Single Instruction Single Data
SIMD - Single Instruction Multiple Data
MISD - Multiple Instuction Single Data
MIMD - Multiple Instruction Multiple Data

High parallelism may not lead to high speed - Depends on granularity.
Granularity - Average number of compute instructions before communication is needed across processor.
If more commnunication is required, Then its good to go with multi processor/computer systems.
If less communication then go with distributed systems.

Amhdal's Law   ---> speed is limited by sequential part of the program.
• T(1) : Time taken for a job with 1 processor
• T(N) : Time taken for same job with N processors
• Speedup S(N) = T(1) / T(N)
• S(N) is ideally N when it is a perfectly parallelizable program, i.e. data parallel with no sequential component
• Assume fraction of a program that cannot be parallelised (serial) is f and 1-f is parallel
• S(N) = T(1) / ( f * T(1) + (1-f) * T(1) / N )   Only parallel portion is faster by N
• S(N) = 1 / ( f + (1-f) / N )

10% of a program is sequential and there are 100 processors. What is the effective speedup ?
S(N) = 1 / ( f + (1-f) / N )
S(100) = 1 / ( 0.1 + (1-0.1) / 100 ) = 1 / 0.109
= 9.17 (approx)

Amdhal's law doesn't consider network/communication delay, context switch, I/O etc..

Gustafson-Barsis Las --- Solver larger problems when you have more processors. Increase the workload with increase in number of processor.
This law is about how much workload can be processed with N processor.

Let W be the execution workload of the program before adding resources f is the sequential part of the workload
So W = f * W + (1-f) * W
Let W(N) be larger execution workload after adding N processors
So W(N) = f * W + N * (1-f) * W
Parallelizable work can increase N times
The theoretical speedup in latency of the whole task at a fixed interval time T S(N) = T * W(N) / T * W
= W(N) / W = ( f * W + N * (1-f) * W) / W
S(N) = f + (1-f) * N
S(N) is not limited by f as N scales


Data Access strategies

1. Replication - replicate all data across nodes of the distributed system
    - Higher storage cost
    - All data accessed from local disk so no runtime communication on network
    - High performance with parallel process
    - Fail over across replicas
  concerns: Keep replicas in sync - various consistency models between readers and writers.

2. Partition - partition the data typically equally to all the nodes of distributed systems
     - High query cost if query needs to access multiple partitions.
     - Works well for tasks/algo is data parallel.
     - Works well when there is locality of reference within partition.
  concerns: 
     - Partition balancing
     - sharding problems
     - How to improve locality of reference

3. Dynamic communication : Communicate only the data that is required.
    cost : high network cost for loosely couupled systems and data set to be exchanged is large
   Advantage: Minimal communication cost
   concern: Highly available and performant network

4. Networked storage: Common storage on the network (SAN, NAS)
      common storage on the cloud - Amazon s3

*******************************************************************************************************************
Lecture 4

Cluser computing
- Peep to peer computing
- clinet server computing
- Cloud and distributed computing
Hadoop cluster architecture


Data Analytics:
  -- Used to analyse the structured and unstructured data to derive meaningfule business advantages.

Descriptive analysis : Provides ability to alert, explore and report mostly using internal and external data
   - Usually data from legacy systems, ERP CRM
   - Relational data from data warehouse
   - Structred and not very large data sets
   Solves questions like
    What happened? and why happened?
    Analytics 1.0

Predictive Analysis: Use the data to predict the future
    -- Based on large, unstructred data gathered over period of time
    -- Analytics 2.0

Prescriptive Analysis : Which use data from past to make predictions and also recommendations
    -- Data is blend from big data and legacy systems
    -- Analytics 3.0 - Descriptive + predictive + prescriptive
    -- What, when, why it will happen and what is the action to be taken?

Big Data analtics : 
    -- Wortking with datasets with huge volume, variety and velocity beyond storage capabilities of RDBMS
    -- Locality of reference - move code closer to data
    -- To gain competive advantage
    -- Batch processing and stream processing.
    -- Lot of technologies available
What is not Big Data system
 * Not to meant to replace RDBMS
 * Depending on usecase need to develop techstack and build big data pipeline.
 * Not a volume game

Adoption challenges:
   -- Executive sponsership for big data analysis activities
   -- Sharing the data across different business units.
   -- Right skillset
   -- To use structured or unstructured data
   -- Storage and processing capbilities - In house or cloud ?
   -- Determining what to do with insights created from big data.

Technology challenges:
   -- Scale - horizontal scaling
   -- Security 
   -- Schema : need to have dynamic schema
   -- Continuous Availability
   -- consistency
   -- partion tolerant
   -- Data quality

*******************************************************************************************************************
Lecture 5

Issues with Map Reduce
- Latency is high as reading and writing scripts from disks
- Poor filt for Iterative applications such as ML and graph algo
- Not in memory computing. Spark is in memory computing.

Reliability - An inverse indicator of failure rate. How soon system will fail.

MTTF - Mean Time To Failure is an Averaged value. 
      Total number of failures / Total number of units
Failure Rate = 1/MTTF
Increase in the MTTF value is the objective.

MTTR - Mean Time to Recovery
     - Total number of hours of maintainance/ Total number of repairs

MTTD - Mean Time to Diagnose
MTBF - Mean Time Between Failures - MTTD + MTTR + MTTF

MTTF reliablity
Serial Assembly system - C = A + B
MTTF of system = 1/SUM(1/MTTFi) for all components i
Failure rate = SUM(1/MTTFi) for all components i

Parallel Assembly system = C = A or B
MTTF of C = MTTF of A + MTTF of B  because both A and B have to fail for C to fail
MTTF of system = SUM(MTTFi) for all components i
Failure rate = 1/SUM(MTTFi) for all component i

MTTF availability
Availability = Time system is UP and accessible/Total time observer
Availablity = MTTF/(MTTD* + MTTR + MTTF) - Unless specified MTTD is zero.
            = MTTF/MTBF
System is highly available when
MTTF is high
MTTR is low

Problem:
A node is cluster failed every 100 hour while other parts never fail. On failure of the node whole system needs to be shutdown, faulty node replaced and system.
This takes 2 hours, The application needs to be restarted which takes 2 hours.

what is availabilty?
If downtime is $80k per hour, what is the yearly cost?
Answer - This is serial assembly
        MTTF = 1/(1/100)
             = 100

        Availablity = 100/ 100 + 2 + 2
                    = 0.961

         yearly cost = 1 - 0.961 = 0.039
                     for 100 hrs/24 hrs = 4.16 days
                      for month 30/4.16 = 7.211 - 7.211 * 0.039 = 0.28125
                      for year 0.28125 * 12 = 3.375 = 3.375 * 80K = $270k
                     
                

Fault tolernace configurations - 

Load Balanced : Active - Active - costly
Hot standby : Active - passive - One is active and other is standby - Takes few seconds
Warm standby : Software installed on secondary and up and running, Once Active is down seconday is configured - few minutes
Cold standby : Secondary node act as backup for an identical primary system - Secondary is installed and configured after active fails - Takes few hours.

Fault tolerant cluster recovery
Diagnosis - heart beat messages
Backward recovery - Rollback to a particular checkpoint.
Forward recovery - Reconstruct the state based on the data.

CAP - Theorem
    A distyibuted system running over a cluster can only provide 2 of the 3 properties.
    In effect, When there is partition, The system has to decide whehter to pick consistency and availability.
   i.e CP or AP because partion is bound to happen in distributed systems.
Consistency: A read of data item from any node results in same data across multiple nodes

Availabilty: A read/write request always acknowledged in form of succes or failure in reasonable time

Partion Tolerance: System can continue to function when communication outages split the cluster into multiple silo and can still service read/write requests

BASE - Base is a database design principle based on CAP theorem.
statnds for 
Basically Available - always repsonsds with success or failure
Soft state - State where db node is not in sync with other nodes.
Eventual consistency - State in which reads by different clients, immediately following a wrtie to database may not return consistent results.



*******************************************************************************************************************
Lecture 6

Level of Consistency

Strict - A read of data item from any node results in same data across all nodes

Linearisable - Allows read by other client while that value is being updated.

Sequential -  Updates to a values should be read by other nodes sequentially. It means x is updated 1st and Y is updated next, Then node reading X and Y should get updated value of X first and then Y.

Causal - Node sees writes to x and then writes to y. Means only casually connected to writes and reads need to be ordered. Example value of X is used to write Y. So all the nodes must observer write to x first then writes to y.

Eventual - If there are no writes for some time then all threads agree on latest value of data item.


Majority RW - Read majority of nodes or write majority of nodes to get causal consistency.

Different types of consistency models
 When Network partition happens there can be many secnarios for fault tolerance. Below are some of the use cases that can be configured in DB / big data systems based on usecase.
Example, Cluster has 9 nodes and due to network partion cluster has divided in to 2 partition one with 5 nodes and other with 4 nodes with new primary node of new cluster.
- causal consistency with durable writes -  Read and write happens with majority of clusters, Read/write to non majority cluster should faile.
- causal consistency with non durable writes - Read happens with majority cluster but write will be succeeded for 1(so it will immediately succeed on non majority cluster) - This write may be rolled back when immediate read operation is done on majority nodes.
- eventual consisitency with durable wirtes - Writes may not be seen immediately but eventually. read local and write majority
- eventual consistency but no durablitiy - read local and write 1 - Again this write may be rolled back as is the case with causal consistency with no durable wirtes.


Big data analytics life cycle
1. Business case Evaluation
2. Data Identification
3. Data acquisiton and filtering
4. Data extraction
5. Data valiation and cleansing
6. Data aggregation and represenations
7. Data analysis
8. Data visualisation
9. Untilisation of analysis results

*******************************************************************************************************************
Lecture 7

TOP DOWN DESIGN 
   -   Sequential - f1 ->f2->f3
   -   Parallel   - f1 ->f2   
                              f4
                       -> f2


Data parallelism - Static partion is done and assign it to number of available processor. Here data partion may be balanced or un balanced.
Tree level parallelism - Divide problem in to subproblem in tree strucutre and assign it to those many processors. (Quick sort)

Task parallelism - Parallel tasks that work on same data

Request parallelism - Scalable execution of independent tasks in parallel. Execute same code but in parallel instances (Client server models)

Some MAP REDUCE Examples

*******************************************************************************************************************
Lecture 8

HADOOP Architecutre 

client server and peep to peer communication

master node and slave nodes

Master Node
   - Name Node - HDFS name space metadata manager
   - YARN cluster level resource manager

Slave Node
   - Data Node - Node level data manager - Stores actual data
   - Node manager - YARN node level resource manager
   - MAP, REDUCE tasks

HDFS features 
   - Distributed java based file system that sits on top of native FS
   - Data is split in to large blocks : 128MB
   - Enables storage of very large files across nodes of Hadoop cluster
   - Scale through parallel data processing
      - 1 node with 1 TB storage with IO bandwidth of 400 mb/s vs 10 node with 10 GB storage with 400mb/s - Second one is faster and scalable
   - Fault tolerant through replication - default 3 replicas
   - Consistency : Write once and read many times
   - cost : Adding more nodes is cost effective
   - Variety and volume of Data - structured, unstructured, semi structured and Hude Terabyte and peta bytes of data
   - Data integrity : Verifys checksum to preserve data integrity.
   - Data locality : Bring computation closer to the data

Client contacts Namenode on Master node and namenode checks meta data and responsds back with Data node information where client has to connect to based on location or availability of data nodes.

NameNode:
   FsImage - contains mapping of blocks to file, hierararchy and file operations/operations
   edit logs - Transaction log of changes to metadata in FsImage - records each change

   - Does not store any data - only meta data
   - HA can be configured
   - Receives heartbeat and block report from all Data nodes in cluster
   - Ensure replication factor is maintained.

   On Startup - 
      - check for status of Datanodes on slaves
      - Gets hearbeat and block report from data nodes
      - check the replication factor
      - Updates meta data
      - Reads FSImages and editlogs from dist to memory
      - Applies edit log changes to FSImages

Secondary Name node - constantly reads all the file system and metadata from RAM of the NameNode and writes to its local file system.
                     - Downloads Edit logs from namenodes and applies it FSImage on regualr check points.
                     - Thhis node takes over when primary is down.
                     - Not Hot standby

HA Config of Name node
   -   Data node sends hearbeat to both name nodes
   - Zookeeper session used for failure detection and election of new active.
   - Journal Nodes/NFS - Write happens Only via Active node then passive name node will read from Journal Node.

Replica placement strategy with Rack awareness
   -   First replica is placed on the same node as client
   -    Second replica is placed on node that is present on different rack
   -   Third replica is places as second but on a different node
   -    For replica factor greater than 3 - 4th replica can be placed randomly which satisfies upper limit per rack 

*******************************************************************************************************************
Lecture 9

MAP REDUCE - Write pipelines and read pipeline

   -   Client sends request for read/write 
   -   Name node receives the request and check its metadata and fetch the datanodes where this block belongs.
   -   Name node then sends these datanodes info to client (DN1, DN2 and DN3 in case of 3 replicas)
   -   client then creates TCP/IP conection to lets say DN1.
   -   client informs DN1 to be ready to receive the block and provides IP of DN2 and DN3 for replication.
   -   With this pipeline will be setup from client to for example DN1 then DN1 to DN2 and then DN2 to DN3.
   -   Then client will start transferring the data to DN1, Then DN1 replicates the data to DN2 and then DN2 replicates data to DN3.
   -   Once the data is transferred, Datanode will update client that data written successfully in reverse manner like DN3-->DN2-->DN1-->client
   -   Client inform namenode that data has been written successfully
   -   Namenode updates metadata and then client shutdowns connection.

Data can be written parallely by setting up multiple pipelines. For ex one pipeline for each block request.
Read request are served by chosing namenodes which is closes to client. This reduces read latency. Reads are parallel.


MapReduce program
   -   RecordReader - reads each record created by input splitter to pass key-valu pair to map
   -   Map : userdefined function to create key, value pair from input key, value pair
   -   combiner: optional reducer for better performance - can be same as reduced code.
   -   partitioner: Takes intermediate output from map and shards it to different reducers.
            -   Ensuers same key is sent to same reducer.
   -   Reduce
      -   shuffle and sort
      -   Reducer

*******************************************************************************************************************
Lecture 10

YARN - Yet Another Resource Negotiotor

 - Key role is to schedule resources in the cluster
 - Takes request from client and talks to node managers for allocation
- creates appmaster
- client can contact appmaster

AppMaster
   - Negoritates resources from resouce manager per application for starting container on nodes.
   - Sends periodic health status of application containers and track progress
   - Talks via Node Manager for updates and usage reports to resource manager
   - Clients can directly talk to appmaster

conatier

Workflow
1. client submits the application/job with specs to start AppMaster
2. RM asks nodemanager to start container and then launches AppMaster  
3. AppMaster on startup registers with ResourceManager(RM). Now client can directly contact to AppMaster
4. As the app executes, The AppMaster negotiotes resouces in the form of conainters vai the resource request protocol invlolving RM
5. As a container is allocated successfully for an app, AppMaster works with the nodemanager on same of diff node to launch the container.
6. App specific code insided the container provides runtime information to the appmaster for progress, status etc..
7. Client can directly communicate with appMaster for progress
8. On complettion of job AppMaster deregisters from RM and shuts down so that containers allocated can be re-purposed.

Capacity scheduler
min capacity, max capacity, user limit factor   -- Go thru the slides

*******************************************************************************************************************
Lecture 11 - Hadoop technology ecosystems

PIG - Gives developer a simpler scripting interface to manipulate data using a higher level programming interface
    - similar to sql
    - pig script is translated to map reduce tasks by the pig runtime and executed on hadoop cluster

    - programs are written in Pig Latin - scripting language

PIG Data model
    Atom - Basic data type like byte, int

    Tuple -  Ordered set of fields of various types, like a row in RDBMS table
      eg : (andy, 25)

    Bag : An unordered set of tuples. A bag can be inside tuple
      eg : {(andy,65), (ram, 23, 6000)}
       inner bag : (1, {(1,2,3)})

    Map : key value pair
      eg : [name#andy, age#25]

    Relation: A bag but not an inner bag which can be inside a tuple. Like table in RDBMS.

HBASE : Key value store
  HDFS provides sequential access to data
  HBASE provides random access capability of large files in HDFS. Hash tables, key value stores
  Built for wide tables with many attributes
  column oriented storage.
  strongly consistent
  can be used when need to lookup data in large data store.

  columnar storage - to leverage spatial locality
    - Is about grouping multiple columns in to column family

HIVE: Data warehouse
   Provides way to process large structured data in hadoop.
   Data warehouse on Hadoop/HDFS
   SQL like query interface - HiveQL
      -   Its not RDBMS
      -   Not for real-time queries with row level updates
   Meant for OLAP type quereies
   Contains execution engine which transaltes Hive query in to map reduce jobs

SQOOP - SQL to Hadoop and Hadoop to SQL
         Cannot ingest continuous stream of data.
         Its limited to structured data.

FLUME - To move large non relational data : streaming data eg. logs
         Event driven
         More reliable

Zookeeper:
   Provides coordination system

Oozie : Scheduler for hadoop jobs
   It maintains metadata.
   Check Master is working fine or not

*******************************************************************************************************************
SQL vs NoSql Database
Mongo db - practice queries
Graph database - Neo4j practice queries - November 3 video

Cloud service for big data - November 5 video (Mostly repetation on cloud IAAS, PAAS, SAAS, Lambda, VPC, s3 object storage, EFS, EBS

5 essential characters of cloud computing:
On demand self- service
Broad network access
Resource pooling, Rapid elasticity, Measured service

4 Deployment models
Public, private, hybrid, community

3 cloud service models : SAAS, PAAS, IAAS

Cloud Services for Big Data
Cloud follows same model as Big Data, both requiring distributed clusters of computing devices.
✓ Cloud Computing considered as ideal platform for handling big data IaaS:
✓ Can provide huge storage and computational power requirements for Big Data through limitless storage and computing ability of cloud computing, e.g. AWS S3, EC2
PaaS:
✓ Vendors offers platforms ready with Hadoop and MapReduce (AWS EMR).
✓ Saves hassles of installations and managements of these environments SaaS:
✓ Great help to organizations which requires specialized software's for big data like for social media analytics, feedback monitoring etc.
✓ SaaS vendors provides out of the box solution for such common use cases


AWS architecture:
• Region: DCs are located in specific regions
• Availability zone: A region is split into AZs to isolate failures. AZs within region are connected with low-latency network.
• Compute instance: A machine, VM, container or function (FaaS). With DNS support, Elastic static IPs. Can be started from images.
• Attached volume: A compute instance is connected to block storage volumes for local FS.
• External storage: To serve as large remote raw data storage. E.g. An object storage like S3
• Security groups: Rules created to control access to the compute and storage instances.
• Load balancing: Service to spray load across multiple compute instances across or within AZ.
• Elastic scaling: Manual or automatically creation / deletion of compute instances.
• Backup/recovery: Snapshots can be taken to preserve state.

Automated elasticity
Applications can be provisioned with resources on-demand when they need it
Re-think application design
– Understand which application components can be elastic
– What is the impact on overall application being elastic

Design for failure (e.g. in AWS)
• Use Elastic IP, a static IP that can be mapped to different servers
• Use multiple AZ (logical DCs) during deployment - create replicas across AZ
• Maintain images (Amazon Machine Images) for replication / cloning, quick restoration
• Usemonitoringservices,e.g.AWSCloudWatch
• CreateAutoScalinggroupstoreplacefailedinstancesquickly
• Takesnapshotsandbackuponexternalstorage,e.g.S3

Decoupling application components
• Have application components interact using queues
• Stateless components as much as possible
• Store any session state in DB

Automation of infrastructure
• Key to auto-scaling .. create auto-scaling groups for different clusters
• Monitor for resources (CPU, mem, I/O) and take actions like using machine
images to launch new instances or sending notifications
• Store config data used for automation in a DB
• Build process : latest binaries go on a global external storage (e.g. AWS S3) - so any instance can spin up using latest build
• Open source config management tools: Chef, Puppet, Ansible etc.
• Build machine images with minimum OS for quick deployment. Configs and and
user details can be passed on and after launch.
• Bootup from 1 or more attached block storage volumes, e.g. EBS volumes attached to EC2 instance
• App components should not depend on location, hardware, IP addresses because the image/binary can be moved anywhere on failure

Security
• Traditional Enterprise perimeter security doesn‟t work on Cloud
• Security needed at every layer of application architecture
• Protect data in motion
– Use SSL certificates and encryption in data movement – VPC to isolate within public cloud
– Secure VPN for connecting on-prem to Cloud instances
• Protect data at rest
– Can encrypt files or entire volumes
– Key management is critical: Make sure keys are secure and not lost
– Snapshot volumes for recovery, e.g. if compromised at some point of time
• Use the Identity and Access Management service (IAM) given by the Cloud provider
• Use instance specific virtual firewalls (e.g. AWS security groups) for compute instances, load balancers, DB etc.
• Use Network ACLs for subnet level access control
• Use Web App Firewall (WAF) along with application level load
balancers to block application specific attacks

*******************************************************************************************************************
STORAGE:

Object storage – S3
• File storage
– Elastic File System (EFS)
• Block storage
– Elastic Block Storage (EBS)

S3 - Object storage
S3 is an object storage
– which means it stores objects which are files with lot of meta-data associated
A bucket is the container for objects
An S3 account can have hundreds of buckets and a bucket can have hundreds of objects A bucket can also have folders to organise objects
An object
– can be 1 byte to 5TB
– is uniquely identified by a developer assigned key and a URL
– has an ACL to control who can access from anywhere - not necessarily from within AWS
– supports versioning and “eventual consistency” across multiple reads / writes – is partitioned and replicated

Consistency model
S3 - Follows AP design wrt CAP Theorem
So when a user uploads an object, it is replicated but readers can access inconsistent replicas and all replicas are “eventually consistent”
Option for reduced redundancy for lower cost - so writes / updates may not be durable and may need to be repeated .
Some observations:
– User uploads object but a reader can get “key does not exist” – An object may not appear in a listing of a bucket immediately – Old data may be returned if a write is not propagated by then – A deleted object may be seen in a listing for some time

Object Lambda
Remember Function-as-a-Service ? What if you could move the function closer to Data ?
- create aws lamda function
- create and configure s3 object lambda access point
- invoke lamda function on the S3 GET request
- Process and transform S3 objects
- Data is returned to the application

Use cases
• Backup and restore for Cloud as well as on-prem data
• Disaster recovery with cross region replication
• Data archival
• Cloud based applications with scalable storage access from anywhere
• Create data lakes for big data analytics

Overview-File Storage
• File system built using SSD storage that can be accessed from multiple Cloud instances or even customer premises servers
– Supports NFS protocol
• Applications can simply attach the file system within VPC - so not
much change to get elastic scalability and IOPS – 10GB/sec, 500K IOPS
– Multiple NFS clients can attach EFS
• Multiple storage classes

EFS Storage classes
Standard - with replication to 3+ AZs
– EFS Standard and EFS Standard - Infrequent Access
One Zone - replication only within one AZ at lower cost
– EFS One Zone and EFS One Zone - Infrequent Access
– 80% files typically in this category
Setup lifecycle policies to move data based on age to Infrequent Access (IA) class

Standard storage class
• AVPC has3AZs
• Each AZ has a mount target
• Typically should access EFS from a mount target within same AZ for performance and cost
• Can create a mount target in one of the subnets within an AZ

One Zone storage class
• Single mount target in 1 AZ
• So instance in another AZ has
to pay for data access cost

Overview
• Block storage for an EC2 instance that can be attached and detached anytime
• Provides a volume as a collection of network attached blocks that are exposed as disks
• Depending on performance / cost one can opt for HDD-backed or SSD-backed volumes
• EBS volumes are durable and replicated within AZ
• Can‟t move a volume to another AZ without snapshot

EBS or EFS
EBS
– Can be only accessed by one instance at a time
– Steady predictable performance for a single instance use case
– upto: 4GBps, 64TB, 260K IOPS, sub-millisec latency per volume
– I/O intensive applications, e.g. relational databases, OLAP engines
– Multiple performance / cost options
– Cheaper than EFS per volume but only for one instance - so effectively more expensive
EFS
– Like a distributed multi-user network file system
– Scalable access across many users with decent performance
– Costs more per GB but is shared by multiple instances - so turns out cheaper for cost sensitive shared storage applications

*******************************************************************************************************************
Big data computing engines - November 6 video
Spark
- Hadoop is slower because of read/write to disks, replications - Suitable for batch processing
- Spark is inmemory, Works on RDD engine - Resilient Distributed Dataset
- Do not use spark for large batch processes with high memore requirements

SPARK - RDD
- spark stores intermediate results in distributed memory without complicating user programming.
- Spill overs can be transperently stored on disk

What is RDD?
- Fundamental data structure in SPARK
- Immutable distributed collection of objects
- Can be created from other RDDs or parrallelist an exisiting collection in dirver program

Why it is called RDD?
Resilient
– A lineage graph of operations helps to reconstruct when a node fails and part of an RDD is lost
Distributed
–Each RDD is divided into logical partitions for parallel computation on cluster
Dataset
–Can contain any type of objects depending on language used

RDD useful when
– dealing with unstructured data – don’t want to impose schema – low level operations on data
– not interested in optimisations done for structured data

RDD features:
Keep data in memory as much as possible
• Evaluate only when an action triggers
• A failed lost RDD partition on a worker can be recovered from lineage of operations
• Cannot change once created
• Persist in memory or storage for
reuse
• Parallelism through partitioning
• Put tasks closer to data location
• Apply operations to entire set of data at coarse grain and not one data item within RDD

RDD limitations:
For structured data
–RDDs do not exploit any optimizers –Better to use DataFrame or DataSets
Since RDDs are in-memory JVM objects there are garbage collection and Java serialisation overheads
Spill over data is put on disks which slows down performance, hence machines need to have enough memory given the data size and analysis

RDD and DSM:
Grain of R/W operation
–RDD is coarse grained as it works at dataset level whereas DSM is at specific data item level
Consistency
–Immutable RDDs are trivially consistent whereas DSM makes sure of consistency if programmer follows a set of rules
Fault recovery
–New RDDs are created on each transformation. So following lineage of operations RDDs can be recovered. DSMs need checkpointing / rollback.
Straggler mitigation: Problem of having slow tasks slow down end to end performance –RDDs make it little easier with backup tasks whereas in DSM it is difficult
Out-of-memory behaviour
–Spill over data goes to on-disk RDDs gradually degrading performance whereas in DSM system swaps may lead to instability

When not to use Spark ?
Large batch processes with high memory requirements
Multi user analysis environments where concurrent demand for memory is high
–May not scale with number of concurrent users

*******************************************************************************************************************
Difference between map() and flatmap()

map() – Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.
flatMap() – Spark flatMap() transformation flattens the DataFrame/Dataset after applying the function on every element and returns a new transformed Dataset. The returned Dataset will return more rows than the current DataFrame. It is also referred to as a one-to-many transformation function. This is one of the major differences between flatMap() and map()
Key points

Both map() & flatMap() returns Dataset (DataFrame=Dataset[Row]).
Both these transformations are narrow meaning they do not result in Spark Data Shuffle.
flatMap() results in redundant data on some columns.
One of the use cases of flatMap() is to flatten column which contains arrays, list, or any nested collection(one cell with one value).
map() always return the same size/records as in input DataFrame whereas flatMap() returns many records for each record (one-many).

Example1:- 
sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()
Output:
[[1, 2], [1, 2, 3], [1, 2, 3, 4]]

sc.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()
Output:  notice o/p is flattened out in a single list
[1, 2, 1, 2, 3, 1, 2, 3, 4] 

*******************************************************************************************************************
Machine Learning

K means clustering:

Euclidean Distance Formula

d = squarerootOf (x2 - x1)^2 + (y2 - y1)^2
x2 - observed value
x1 - centroid value

y2 - observed value
y1 - centroid value

Dataset
Candidate	Weight	Glucose level
1             	72 	185 
2             	56 	170 
3 	            60 	168 
4 	            68 	179 
5 	            72 	182 
6 	            77 	188 
7             	70 	180 
8 	            84 	183 

calculating K means where k = 2

Initial centroid value k1 = C1 - (72, 185)
Initial centroid value k2 = C2 - (56, 170)

Applying formula:
calculating centroid value for C3

w.r.t K1
C3 = squarerootof ((60-72)^2 + (168 - 185)^2)
   = squarerootof (144 + 289)
   = 20.80

w.r.t K2
C3 = squarerootof (60-56)^2 + (168-170)^2
   = squrerootof ( 16 + 4)
   = 4.4721

4.47 is lesser, So C3 will go into k2 cluster.

Now, K1 - (C1)  K2 - (C2, C3)
Update the value of K2 since it has new candidate.

New K2 will be - (C2+c3)/2 = (56 + 60)/2 and (168 + 170)/2 = 55, 169

so, K1 - (72, 185)  K2 - (55, 169)

Further calulation like C4 will be based on above values.
