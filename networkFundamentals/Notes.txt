


Data plane - local per router function. Determinse how data arriving on router input port is forwarded to router output port by examining the headers.
              This has look up table which is populated by control plane. This just looks the look up table to send to the next destination.

              - packet buffering, packet scheduling, Header modification and forwarding.
              - If packet info not in forwaring table, It communicates vertically to control plane

Control plane - Network wide logic determines how data is routed among routers along end to end path from source to destination.
                  This has 2 approaches
                   a. Traditional routing algorithms implemeneted in routers.
                   b. Software Defined Network (SDN) implemented in remote servers.

Management plane - Network adming configures and monitor switch through this plane.

Traditional routing algorithms.

1. link state algorithm - Djkstra's shorted path algorithm
    - This alogorithm runs based on the link cost. Each node in the network will advertise its link costs to every other node in the network.
      So, At the end it will have global link state (or Graph) snapshots which shows the link cost between every node to every other node.
      Once we have this link state in all the routers, Then Djstra's algorithm will be run to find out shortest path between any 2 nodes.
      Then this data is feeded to routing look up table in data plane.

2. Distance vector algorithm - Bellman ford's algorithm  - Not so popular and less used.
    -  In this algo, Every node advertises only to its adjacent node about the best path to reach other nodes from itself.
        example : A --- B --- C
                        |     |
                        D --- E
          In this example, B advertises to A that best path to reach to E is D if link cost from B-D-E is less compared to cost B-C-E
        This way it calculates distance and advertise it to other nodes.
        This algorithm is not widely used not because of some convergence problems.

These traditional algorithms are not suitable for large networks on the internet. These are mostly suitable for LAN or small autonomus networks.


**** Routing protocals on the internet *****
 Autonomous Systems - Small network or domain managed by only 1 admin.

There are 2 ways of communications here
1. - Intra Autonomous - communication that happens within the AS
2. - Inter Autonomous or domain - Communicatinon that happens between AS. Example 1 ISP to another ISP. This is how internet is connected with network of networks.


In order to have links over larger networks, There are 2 ways how routing tables are populated.

1. Intra domain communcation - This is done via link state algorithm
2. Inter domain communication - This is done via BGP (Border Gateway Protocol)

Border Gateway protocols are of 2 types.
1 - iBGP - Intra BGP - Protocal that runs within the autonomus systems to advertise external autonoums system path to every node in the AS.
2. - eBGP - external BGP - Protocal that runs thru gateway router of one AS to other As to advertise its path to gateway router in other AS.
            This advertisement is done via subnets.
            example : AS1 can have subnet of 198.18.*.* and AS2 can have subnet of 198.21.*.*
            Now AS1 can advertise AS2 with its gateway router that it can be reached via subnet 198.18.*.*.
            By this AS2 will learn that all the packets which have destination ip staring with 198.18 will have to go to AS1 via its gateway router.

Internet is scalabe because of this aggregation of networks with subnets.

Now Routing table of AS2 has entries like
AS1 - Gatway router of AS1   ----- This is found out with eBGP
Gateway router of AS1 - can be reached via some internal router say D  ---- This is found out linkstate algorithm


********************************************************************************************************************

Transport layer protocols.

UDP - connectionless, non reliable, out of order packets, no congestion/flow control, Fast
      If application needs packets in order it has to handle by its own.
      Useful in streaming

TCP - Connection oriented(Three way handshake), Reliable, In order packets, Less faster comapred to UDP, cumulative ack,
      Flow control and congestion control is handled in tcp layer itself.
      Retransmission of packets based on ACK.
      Works based on slinding window - Maximum n packets can be sent over the network for which ack is not received yet. Once ACK is received 
      for x bytes window will be moved to x bytes further and it can send x bytes more.

Flow control - If sender is sending packets faster than what receiver can take then packets will be lost at the receiver end as application is slow to process requests.
               Receiver end socket has buffer up to which it can hold the data, Once the buffer is full packets will be dropped and it causes retranmission of packets.
               To solve this TCP can send receiver window size, Based on this sender restricts to send only that many bytes and this avoids retranmissions.

Congestion control - Too much of traffic arriving from different source and converged at particular node causing congestion.

********************************************************************************************************************

SDN (SOftware Defined Network)

Data plane functions is implemented in each of the router but control plane functions are implemented at the remote centralized server.
remote server make decisions on path and communicated to each router and forwarding tables are populated.

CA - control plane agent in each router talking to remote server.

SDN is desinged to handle east - west traffic, changing network in Data center network
Because of changing network - Programable solution is required.

Take high path functions(Data plane functions) and implement them in highly efficient hardwares/network switches. 
These are physical SDN enabled switches(Open flow switch) where data plane is implemented but control plane is moved to some centralised server.

This enables higly efficient control centre software running in Data center and push the routing data to data plane of network switches.

Here only SDN controller alone need to have network topoloy vs earlier model needs to have every router needs to have network topology to run link state algorithm.

Routing, Load balancing, Firewall can be implemented and communicated to SDN via rest based api.

Each router contains forwarding table - 
   - Match plus action abstraction : match bits in arriving packet, take action.
      - dest based forwarding - forward based on dest ip address.
This is called generalized forwarding - Any incoming traffic can be matched to any action like drop/copy/modify/log packet.
All this generalized forwarding is pushed to forwarding table by SDN remote server which programs this or configures.
Data plane blindly looks at this forwarding table and takes action.

So basically SDN helps to define many other actions apart from routing like load balancing, firewall or define any other rules etc..

********************************************************************************************************************

Network Virtualization

- In computing, Network virtulization is the process of combining hardware and software network resources and network functionality in to a single software based admin entity, a virtula network.
Virtualizes the physical network with virtual networks.

Network Function Virtualization [Virtualised Network model]
- Is the software implementing network functions like firewall, routing, load balancing that run on any underlying well known hardware or VMs. Its not bound to particular hardware.
   Traditionally each of these network functions are bound to respective particular hardware. So, One physical node per role.

VNF(Virtual Network Functions) - Implemented by software vendors who doesn't have hardware experience. These VNF run on hypervisors.


SDN and NFV are each independent technologies but they can work in tandem in Data center network.
NFV - Provisions implementaion of VNF( Firewall, Antivirus, LB etc with underlying data center hardware)
SDN - defines the path, For a give traffic what are the VNF it has to go thru.

********************************************************************************************************************

DATA CENTER NETWORK

Physical compute and storage servers are connected via switches in a huge data centre.
Servers are arranged in the form of racks and each rack has one Switch which is called Top Of Rack switch.
This switch will connect servers in this rack with others servers in different network.
So, Within data centre there is high volume of network and connectivity between each servers are needed as application are evolving from monolith to distributed and microservices.
With these modern applications more than North south traffic(client server traffic) East west traffic is more signigicant(i.e server to server). Because of microservices a client
requesting a web page has to go thru multiple servers to fetch the data and respond it back to clients.

Because of this East to West traffic SDN switches are extermely helpful for cost optimisation and speed. 
Instead of having 1000 switches we can have 10 SDN servers which can handle 1000 SDN enbabled switches which just handles data plane functionality via Openflow protocol.
SDN enabled switches are less costly compared to traditional switches.

Because of the growing demand from East to West traffic there are many challenges in DCN.

1. High volume of traffic
   causes network fault and shold provide auto recovery mechanism.
    - Number of managed objects in cloud DCN is huge because of NFVs
    - Network needs to detect dynamic VM migrations and elastic scaling of applications.
     - Boundries are blurred and its very difficult to locate and isolate issues
    connections faults, performance faults such as congestion and policy faults are some of Network faults.
Addressing network faults.
- use of self healing networks.
- Use of intelligence analysis engine to predict/detect/isolate network faults.
- Use of SDN and SDN controller for simplified cloud network operations.
- Bandwidth oversubscription.

TCP Incast
- N/w pathology that affects many to one communication pattern in datacentres.
  leads to lot of retransmission.
Its fundamentally transport layer problem and best solution can be found at TCP level

Infrastucre challenges.
- DC network cost
- DC cooling
- DC cabling


DCN evolution - 
Traditional network topology - access-aggreation-core
well suited for nort and south traffic
only layer2 network hence only need switches
easy to use and faster as there are less server to server traffic.
This can not scale for modern DC netowrk as server to server n/w huge and its causing brodcas storming
VLAN limitations - 4096 vlans
STP limitations - need more aggregation switches.

Modern DCN topologies- Clos topology
leaf-spine topology - Mesh topology
Fat tree topology

********************************************************************************************************************
Each leaf connects to each spine.
Its entirely based on IP routing layer3 network. So, Its scalable and reduntant.
Leaf and spine are of the same type. layer 3 network and uses layer 3 routing protocols.
Adding spines increases the capacity between lear nodes
Classic 3 stage clos topology
if m=n=t --- Leaf spine topology

DCN Design aspects
- Choice of topology
- Oversubscription of bandwidth
- Multipath routing - To reduce the cost and server latency
- overall cost

DCN Technology evolution
  Access-aggregate-core - Spanning Tree protocol
  Virtual Chassis Technology - Impmlement N:1 virtualisation
      Integrates the control planes of multiple devices to form a unified logical device
      challenges - scalability, reliablity, upgrad challenge, Bandwith waste

  Layer 2 multipathing technology
    popular in WAN
    The basic principle of L2MP tech is to introduce mechanisms of routing technologies used on Layer3 networks to layer 2 networks.
    Adds the addressable identifier to each device similar to IP on layer 2 
    TRILL - protocol is standardised by IETF


TRILL
- Transparent Interconnection of Lots of Links
- Is implemented by devices called TRILL switches
- Trill combines techniques from bridging and routing, and is the application of link state routing to L2 networks
- TRILL uses MAC in TRILL in MAC encapsulation
  i.e in addtion to the original etherenet header, a TRILL header that provides an addressing identifier and an outer Ethernet header
  used to forward a TRILL packet on an Ethernet network are added.
  outer MAC header - To send to next hop
  TRILL header  -  To perfrom link state routing(layer3 routing) to avoid loop
  Inner mac header - Original ethernet header

Disadvantages:
  - TRILL used VLAN IDs to identify tenants. TRILL network supports only about 4000 tenants.
  - A solution to the tenatn problem was considered at the beginning of TRILL design. A field was reserved in the TRILL header for tenant
    identification. Not used reserved field because this protocol not used or evolved further
  - Increased deployment costs : Because of specialised TRILL switches. Requires upgrad of forwarding chips.


********************************************************************************************************************
    
Overlay Network: https://info.support.huawei.com/info-finder/encyclopedia/en/Overlay+network.html
- A software defined logical network built over an existing underlay network
- Is completely decoupled from the underlay network
    - Allows the underlay network to be flexibly expanded
    - Facilitates SDN architecture deployment

NVO3 network: Network virtualisation over layer3  network
 - is a tunnel encapsulation technology that encapsulates layer 2/3 packets over tunnels(layer3)
 - NVO3 includes VXLAN.
 - VPN is one example of tunneling where it hides user's private IP address and connects to its server with over VPN.

VXLAN:
- Is the implementaion of NVO3 technology.
- It encapsulates layer2 packet in to layer3 tunnel.
- This tunneling is done by VTEPs (Edge switches or virtual switches in case of VM)
- layer2/3 packet acts as appliation data to VTEP and it doesn't care what it has. It only creates tunnel between VTEP1(at source network)
  VTEP2(destination network)
- VTEP1 encapsulates layer2/3 packet coming to it with VXLAN header and send it over layer3 network to VTEP2 and VTEP2 decapsulate it and send the underlying layer2/3 packet to actual destination.
- The VXLAN encapsulation invovles giving unique identifier to VM at VTEP1 and VM at VTEP2 whcih is called VNI(virtual network identifier)
- While encapsulating VTEP1 puts this VNI in VXLAN header, Then that is again encapsulate with UDP header(fixed udp port for vxlan), Then
  that is again encapsulated in outer IP header. Then this is again encapsulated with MAC header as it does in normal TCP/IP protocol.

Why VXLAN is better than VLAN?
- VLAN supports only 12 bits hence 4096 logical ports.
- VXLAN on the other end use 24 bits which server 16 billinon connections.
- VXLAN provides lot of flexibilty because of overlay IP network.
- Can add software defined Value added service(like firewall) easily

Control plane in DCN underlay network:
- is similar to control plane in IP network.
- Protocol used are OSPF for smaller Data center Network and BGP for large networks
- MGBGP is used by creating autonoums systems inside the data center network.

How control plane works in overlay networks.
- There are 2 widely used protocols that used in VXLAN to advertise the links.
  1. Flood and learn multicast based control plane
  2. VXLAN MPBGP EVPN control plane.

1. Flood and learn multicast based control plane
- This is done similar to the layer2 brodcasting of mac addresses.
- Ecah overlay network creates multicast group with all the VTEPs that contains atleast 1 VM which is of that VNI.
- Instead of Brodacast, It uses multicast on layer 3 network.
- VM in one VTEP will brodcast request asking for MAC address of given IP address. ARP request
- VTEPs belongs to same multicast group will get the ARP request.
- VMs in other VTEP will unicast back saying this IP address belongs to this MAC and also it puts the entry in the routing table.

2. VXLAN MPBGP EVPN control plane.
- EVPN : Ethernet Virtual private network
- Creates tunnels between each network(overlay) with underlying network.
- extension of BGP network
- Advertises its private IP addresses to other edge with in its overlay network only not to other networks.
- It avoids flooding hence used in large sized DCN networks (for more than 100 VMs)

VXLAN MPBGP EVPN Control Plane
• The Flood-and-Learn method creates significant flooding traffic resulting in network expansion difficulties
• As a solution to these problems, EVPN is introduced on the VXLAN control plane
• EVPN = Ethernet Virtual Private Network OR Ethernet VPN
• By referring to the BGP/MPLS IP VPN mechanism, EVPN defines several types of BGP EVPN routes by extending BGP.
• The PE node role described in BGP MPLS EVPN is equivalent to the VTEP/network virtualization edge (NVE) device
• It advertises BGP routes on the network to implement automatic VTEP discovery and host address learning.

  EVPN Advantages
• Using EVPN on the control plane offers the following advantages:
• VTEPs can be automatically discovered and VXLAN tunnels can be automatically established, overall simplifying network deployment and expansion.
• EVPN can advertise Layer 2 MAC addresses and Layer 3 routing information simultaneously.
• Flooding traffic is reduced on the network.

MP-BGP
• Traditional BGP-4 uses Update packets to exchange routing information between peers
• An Update packet can advertise a type of reachable routes with the same path attributes, placed in Network Layer Reachability Information (NLRI) fields.
• BGP-4 can manage only IPv4 unicast routing information
• As a solution, Multiprotocol Extensions for BGP (MP-BGP) was developed as a means to support multiple network layer protocols, including IPv6 and multicast.
• MP-BGP extends NLRI fields based on BGP-4.
• After extension, the description of the address family is added to the NLRI fields to differentiate network layer protocols.
• These include the IPv6 unicast address family and VPN instance address family. • The EVPN NLRI defines different types of BGP EVPN routes

  For completeness – the VXLAN Data Plane!
• Depending on the traffic flow direction and scope, DCN traffic can be classified into east-west traffic (transmitted within a DC) and north-south traffic (sent across the DC)
• Traffic transmitted within the same subnet of a VPC is forwarded by a TOR switch after Layer 2 VXLAN encapsulation.
• Traffic transmitted between subnets of the same VPC is forwarded by a TOR switch based on Layer 3 routes. This is done after Layer 3 VXLAN encapsulation.
• Traffic transmitted between VPCs is forwarded across subnets, and isolation for security purposes is required. Therefore, to meet this, the Traffic needs to pass through a firewall and reach the Layer 3 VXLAN gateway.
• Traffic sent from a user outside the DC to a server in a VPC passes through the Intrusion Prevention System (IPS) or firewall, LB, VXLAN gateway, and TOR switch before reaching the server.

********************************************************************************************************************
Multi DC networking

1. Single VPC can span across multiple Data centres.
2. VMs within VPC communicate at layer2
3. There are multiple uscases why we need VPC in Multiple Data centres.
    - Due to nature of applications it is hosting
    - Application or use cases served by cluster which spans across multiple DC
    - Content Delivery networks
      To provide best user experince CDN provider edge servers to host the contents along with central server
      CDN server selection strategy:
      - Based on Geographical proximity to the client via Local DNS mapping (This is static and has disadvantages)
      - Current traffic conditions 
      - IP anycast : Technique to find best/shortest path for given IP when multiple cluster having the same IP
          Routers in the internet route the client's packets to the closes cluster as determined by BGP
          CDN company assigns the same IP address to each of the clusters

Multi DC networking requirements:
1. L2 Network across DCs - VMs within VPC communicate at layer2
2. L3 networking across DCs
3. Data synchronisation and backup. Storage interconnection

1. L2 Networking across DCs
option 1: Virtual private LAN service
option 2: Use of VXLAN preferred

2. L3 networking across DCs (tunneling L3 overlay packet over L3 network)
option1 : MPLS L3VPN
option2: VXLAN based L3VPN service - Is the preferred technology

********************************************************************************************************************

VM, VNF(Virtual Network Functions) and container networking

Virtual switch : Software based switch resides in hypervisor kernel providing local network connectivity between VMs

Open vSwitch(OVS): The OVS is multilayer virtual switch implelemented in software.
                   It behaves like physical switch, Only virtualised.
                  It uses virtual network bridges and flow rules to forward packets between hosts.
                   Supports VXLAN
                  open source apache licensed

OVS component architecutre:
Data plane(fast path) runs in kernel mode
Control plane run in user mode (OVS vswitchd)

OVS Flow forwading: (check November 4 recording)
1. First packet is not found entry in data plane and hence go to control plane.
2. Control plane will then connects to SDN to get the routing information
3. Then it makes the entry in data plane.

Performance challenges with VSwitch:
- In Non virtualised environments, the data traffic is received by the physical NIC(pNIC) and is sent to an application in the user space
  via the kernel space.
- In virtual environment, physical NICS, vNICS, hypervisors and virtual switch in between.
- The hypervisior and vSwitch take the data from the Physical NIC and then send it to the virtual machine or VNF then to application.
- The virtaul layer causes virtualisation overhead and additional packet processing that reduces I/O packet throughput
- VNF(virtual Network Functions) running on VM will face lot of performance bottlenecks

Overcoming OVS bottlenecks:
- Avoid virtualisation overhead and increase packet througput.
- PCI passthrough
- SR-IOV
- OVS-DPDK

PCI Passthrough - Peripheral Component Interconnect(PCI) gives VNF's direct access to physical PCI devices that seem and behave as if they were physically connected to the VNF.
- Can be used to map a single pNIC to single VNF making the VNF appear directly connected to the pNIC.
- Using passthrough provides performance advantages, such as:
  - One to one mapping between VNF to pNIC
  - Bypass hypervisor
  - Direct access to I/O resources.
  - Increased throughput
  - reduced cpu utilization
Drawback:
Using PCI passthroguh dedicates an entire pNIC to a single VNF. It can not be shared with other VNFs. Therefore it limites number of VNFs to the number of pNICs in the system.


SR-IOV: Single Root I/O virtualisation
- is a PCI passthroguh process feature that enables multiple VNF's to share the same pNIC.It emulates multiple peripheral component interconnect express devices, such as pNICs, on a single PCIe device.
- Emulated PCIe device is referred to as virtual function(VF) and the physical PCIe device is referred to as Physical function(PF)
- Physical function has full PCIe features
- VF have simple PCIe functions that process I/O only.
- VNFs have direct access to the VFs

OVS-DPDK(Data plane development kit)
- It opereates in user space
- OVS is upgraded to DPDK libraries to address performance impack of interruptions on packet thorugput because of interrurps to kernel
-  DPDK pNIC will not tirgger interrrupt to kernel(like in usual way) when it receive the packet/data.
- DPDK PMD(Pole Mode Driver) sitting in OVS(user space) will continuosly poll pNIC for the incoming data.
- By this way it will bypass kernel space
- DPDK PMD needs one or more cpu cores dedicated to polling and processing incoming data.
- When packet enters the OVS, It is already in user space. Therefore it can be directly switched to the VNF.

********************************************************************************************************************
Container Networking

use the concept of cgroups (to manage and control resources used by container)
and namespaces to isolate one conatainer with another.
net-namespace is what is used to isolate the networking in containers.

Below drivers exist in docker

- Bridge
- Host
- Overlay
- Ipvlan
- Macvlan
- none

container networking has 4 operating modes

No network : When container is no need to communicate with outside world

Host network: Shares the network namespace with the host OS. Cotainer sees everything that host sees with no isolation.
  - Normally run in privilaged mode
  - Container can modify host OS network state such as routing table, Mac table
  - Since container shares the same network state its difficult to have two containers using the same TCP/IP port number to communicate with outside world

Single host network: Is the defult mode provided by docker.
  - In this mode cotainer can communicate to other container on same node and also to outside world
  - This is served by BRIDGE networking.
  - When docker service is instantiated, It creates docker0 device which is linux bridge
  - Docker creates a veth pair of interfaces, assigns one of them to containers netns and other to docker0 bridge.
  - Mulitple containers created on host can communicate with each other because one of veth interface in each of these conatiners is connected to docker0 bridge

  - docker uses default subnet of 172.17.0.0/16 to the docker0 bridge
  - it assigns 172.17.0.1 to docker0 itself and other available ips to new containers.
  - dockers adds default route in the containers namespace that specifies the next hop as the docker0's ip address
  - docker by default also configures any packet from bridge destined to the outside world undergo NAT so that multiple containers can share a single host IP address to communicate with entities other than the host.

MACVLAN : is alternative to bridge for single host container communication.
  - The kernel driver assigns each MacVlan interface unique Mac address
  - The kernel delivers incoming packet to MacVlan interface whose mac address matches the packets destination MAC addresss and there by correct container.
  - The containers associated with MacVlan network need to be assigned ip addresses in a subnet associated with the upstream interface


Multi host container networking:
- docker defines network type called overlay for multi host connectivity.
- This overlay network is VXLAN(MAC-UDP encapsulation)
- This is L2 network. In other words, the containers spread across the hosts are in the same subnet
- Cordinating IP addresses across multple host which belongs to same subnet is not easy.
- Hence docker uses docker swarm to manage these ip address across multiple hosts.
- communication between conainer in host1 and host2 happens via VXLAN tunnel.
- Docker sets up two interfaces inside a container in this case.
- one for the communication between containers within same subnet using the overlay
- the other one for outside world
- This new interface is also veth but connects to differen bridge called docker_gwbridge.
- packets going thru docker_gwbridge undergo NAT by default just as in case of docker0 bridge

********************************************************************************************************************

Cloud network security:

standard firewalls check for sourceip, destination ip and tcp/udp port without furhter deep packet analysis

Next generation firewall : check for packet payload and check contents like credit card numbers of private data etc..
Conent ID is one example of next gen firewall
- Check for data like credit card
- Threats like viruses, spyware
- URL - checks for malicious URLs like web filtering

IDS/IPS - Intrusion Detection Systems and Prention systems

IDS
- helps detects shift from normal behaviour (suspected behaviour)
- It just detection and there is no action
- Used to just observe the behaviour and notify. Example so and so traffic is going and coming in to the system and notify IT admins.

IPS
- similar to IDS but it also take action to mitigate malicious traffic
- IPS sits at the perimeter to avoid malicious requests coming in to the system

IDS/IPS uses  behaviour, anamoly and signature methods to detect and mitigate attacks

Behaviour analysis:
  -  Analyse behaviour of the network by baselining how network behaves normally
  -  Any deviation from the baseline bacomes an outlier and triggers IPS/IDS based on behaviour deviation

Anamoly detection:
- Malware writers often attempt to masquerade their application as a legitimate application.
- This is method where protocol is correct but there is some manipulation to packet
- This requires packet analysis

signature based detection:
- Understand the pattern of attack
- It works with only recognised and know patterns
- So it detects the most common and generic attacks
- It can not detect patterns which is not yet recognised
- Can yeild false positive

Securing Network services:
- DNS, email and file transfer are internet services

How DNS works?
Every DNS may not have all FQDN.
DNS collaborate with other DNS servers when it doesn't have reuqested full qualified domain name.
It updates its database by collaboring with other DNS

DNS service security 
  - If DNS service is down due to DDOS attack then its as good as internet is down
  - DNS posioning is way of redericting to malicious website when requested for legitimate website
  -  DNS services authenticate to each other by showing their identity.

email security:
- spam filtering
-  installing spam filter security appliance

Network segmentations:
- creating segments and isolate them
- create firewalls between segments
- 
********************************************************************************************************************
Network observability

Distributed systems are hard to understand, hard to control and always frustating when things go wrong.

Observabiltiy represent the operator's lastest attempt to respond adequately to the questsions.
Along with automation, observability has become one of the central pillars of the cloud native data center.

It is defined as the property of a system that provides answers to questions ranging from the mundane to the existential
more precisely, It is a way for an operator to understand what's going on in a system by examining the outputs provided by the system

How observability is different from monitoring?
- monitor, alert, support root cause investigation, support diagnosis
- monitory tells you whether the system works, obervability lets you as why its not working

SNMP - Simple Network Management Protocol
- To mamage the network by pulling the information from network elements.
- Issue with this is that it is reducing the performace of Network elements

Understanding importance of observabiltiy

The challenges of network observability

Network observability in DCNs
- Multipathing : multipaths in the server to server network
- obscured by tunnels : Due to virtualisation, no popular implementation of the traceroute command can reveal the multiple paths via underlay network
- containers and microservices: Network observability tools today are siloed between compute and network
-  Due to short live of containers, makes it harder to perform post mortem analysis

Factors that makes observability easier in DCNs
- Simpler nodes - frees up network to be build entirely with sing kind of devices in clos topology
- network disaggregation : switches are built as platforms not as appliances. Not tied up with hardware.

Observability can be decomposed in to
- Telemetry or gathering data
- storage
-  monitoring
- Altering
- data analysis
